ğŸŒ Fine-tuning Transformer for Low-Resource Languages (ALT Dataset)

Dá»± Ã¡n nÃ y táº­p trung vÃ o viá»‡c nghiÃªn cá»©u vÃ  cáº£i thiá»‡n cháº¥t lÆ°á»£ng dá»‹ch mÃ¡y cho cÃ¡c cáº·p ngÃ´n ngá»¯ Ã­t tÃ i nguyÃªn (Trung-Viá»‡t, Khmer-Viá»‡t, LÃ o-Viá»‡t) báº±ng phÆ°Æ¡ng phÃ¡p Transfer Learning. Dá»± Ã¡n Ä‘Æ°á»£c fork vÃ  phÃ¡t triá»ƒn tiáº¿p ná»‘i tá»« toolkit MultilingualMT-UET-KC4.1.

ğŸ“– Giá»›i thiá»‡u chung

Má»¥c tiÃªu cá»‘t lÃµi cá»§a Ä‘á» tÃ i lÃ :

Huáº¥n luyá»‡n thÃ nh cÃ´ng mÃ´ hÃ¬nh Transformer sÃ¢u trÃªn cáº¥u hÃ¬nh pháº§n cá»©ng háº¡n cháº¿ (GPU 4GB - 8GB VRAM).

á»¨ng dá»¥ng ká»¹ thuáº­t Fine-tuning tá»« cÃ¡c mÃ´ hÃ¬nh Pre-trained Ä‘á»ƒ vÆ°á»£t qua rÃ o cáº£n thiáº¿u há»¥t dá»¯ liá»‡u cá»§a cÃ¡c ngÃ´n ngá»¯ khu vá»±c ÄÃ´ng Nam Ã.

ğŸ›  CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

Äáº£m báº£o há»‡ thá»‘ng cá»§a báº¡n Ä‘Ã£ cÃ i Ä‘áº·t Python >= 3.8 vÃ  há»— trá»£ GPU (CUDA).

# 1. Clone repository
git clone [https://github.com/chiendz11/NMT.git](https://github.com/chiendz11/NMT.git)
cd NMT

# 2. CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n phá»¥ thuá»™c
pip install -r requirements.txt

# 3. CÃ i Ä‘áº·t cÃ´ng cá»¥ Ä‘Ã¡nh giÃ¡ chuáº©n SacreBLEU
pip install sacrebleu


ğŸ“Š Dá»¯ liá»‡u thá»±c nghiá»‡m

Dá»± Ã¡n sá»­ dá»¥ng táº­p dá»¯ liá»‡u ALT (Asian Language Treebank) Ä‘Ã£ qua tiá»n xá»­ lÃ½ tÃ¡ch tá»« vÃ  BPE (Byte Pair Encoding):

Nguá»“n (Source): Tiáº¿ng Trung (.zh), Tiáº¿ng Khmer (.km), Tiáº¿ng LÃ o (.lo).

ÄÃ­ch (Target): Tiáº¿ng Viá»‡t (.vi).

ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng

1. Huáº¥n luyá»‡n (Fine-tuning)

Sá»­ dá»¥ng cáº¥u hÃ¬nh YAML Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho bá»™ nhá»› GPU tháº¥p (Batch size nhá», tÃ­ch lÅ©y gradient).

python -m bin.main train --model Transformer \
    --model_dir ./models/alt_lo_finetune_transformer \
    --config ./config/alt_finetune_lo_prototype.yml


2. Suy luáº­n (Inference)

Dá»‹ch vÄƒn báº£n tá»« ngÃ´n ngá»¯ nguá»“n sang tiáº¿ng Viá»‡t báº±ng mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n.
LÆ°u Ã½: Code Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ tá»± Ä‘á»™ng táº¡o thÆ° má»¥c lÆ°u káº¿t quáº£ náº¿u chÆ°a tá»“n táº¡i.

python -m bin.main infer --model Transformer \
    --model_dir ./models/alt_lo_finetune_transformer \
    --features_file ./data/ALT_Laos/test.bpe.lo \
    --predictions_file ./data/predictions/predictions_lo2vi_transformer_finetune_alt


3. ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng (SacreBLEU)

Sá»­ dá»¥ng thÆ° viá»‡n chuáº©n SacreBLEU Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a báº£n dá»‹ch so vá»›i nhÃ£n gá»‘c (Reference). KhÃ´ng dÃ¹ng script Perl cÅ©.

CÃº phÃ¡p:
sacrebleu [File_ÄÃ¡p_Ãn] -i [File_MÃ¡y_Dá»‹ch] -m bleu -b -w 4

Lá»‡nh máº«u:

sacrebleu ./data/ALT_Laos/test.bpe.vi \
    -i ./data/predictions/predictions_lo2vi_transformer_finetune_alt \
    -m bleu -b -w 4


ğŸ“ˆ Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c

Viá»‡c Ã¡p dá»¥ng Fine-tuning giÃºp cáº£i thiá»‡n Ä‘iá»ƒm BLEU vÆ°á»£t trá»™i so vá»›i viá»‡c huáº¥n luyá»‡n tá»« Ä‘áº§u (Train from Scratch):

Cáº·p ngÃ´n ngá»¯

Baseline (Scratch)

Fine-tuning (Pre-train)

Cáº£i thiá»‡n (Î”)

Trung â†’ Viá»‡t

18.91

22.06

+3.15

Khmer â†’ Viá»‡t

24.42

26.46

+2.04

LÃ o â†’ Viá»‡t

18.41

22.07

+3.36

ğŸ“ Káº¿t luáº­n & HÆ°á»›ng phÃ¡t triá»ƒn

ThÃ nh tá»±u: Chá»©ng minh Transfer Learning cá»±c ká»³ hiá»‡u quáº£ cho ngÃ´n ngá»¯ Ã­t tÃ i nguyÃªn. Tá»‘i Æ°u hÃ³a pipeline huáº¥n luyá»‡n thÃ nh cÃ´ng trÃªn GPU phá»• thÃ´ng.

Háº¡n cháº¿: CÃ²n gáº·p lá»—i dá»‹ch sai vá»›i cÃ¡c thuáº­t ngá»¯ chuyÃªn ngÃ nh háº¹p (hÃ nh chÃ­nh/khoa há»c) do dá»¯ liá»‡u ALT cÃ²n háº¡n cháº¿ vá» miá»n tá»« vá»±ng.

TÆ°Æ¡ng lai:

Triá»ƒn khai Back-translation Ä‘á»ƒ tá»± Ä‘á»™ng hÃ³a má»Ÿ rá»™ng dá»¯ liá»‡u (Data Augmentation).

Ãp dá»¥ng Quantization (INT8) Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ dá»‹ch trÃªn CPU.

ğŸ¤ ThÃ´ng tin liÃªn há»‡

Sinh viÃªn thá»±c hiá»‡n: BÃ¹i Anh Chiáº¿n

Giáº£ng viÃªn hÆ°á»›ng dáº«n: TS. Tráº§n Há»“ng Viá»‡t (thviet@vnu.edu.vn)

GitHub Collaboration: ÄÃ£ má»i thviet79@gmail.com lÃ m cá»™ng tÃ¡c viÃªn.