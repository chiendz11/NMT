# ==============================================================================
# OPTIMIZED CONFIG FOR IWSLT EN-VI (133K) - GPU 8GB
# ==============================================================================

# 1. DATA CONFIG
data:
  train_data_location: data/iwslt_en_vi/train
  eval_data_location:  data/iwslt_en_vi/tst2013
  src_lang: .en 
  trg_lang: .vi 
log_file_models: 'model_iwslt_8gb.log'
lowercase: false 
build_vocab_kwargs: 
  min_freq: 2    # <--- SỬA: Giảm xuống 2 để model học được nhiều từ hiếm hơn (vì data ít)

# 2. MODEL PARAMETERS (Chọn cấu hình "Small" nhưng sâu)
device: cuda
d_model: 256     # <--- SỬA: 512 quá nặng cho 8GB, 256 là tối ưu cho 133k data
n_layers: 6
heads: 8         # 256/8 = 32 dim/head (Tiêu chuẩn)

# 3. INFERENCE
eval_batch_size: 8  # <--- SỬA: Tăng lên để validate nhanh hơn (nhưng đừng để 64 kẻo OOM lúc Beam)
decode_strategy: BeamSearch
decode_strategy_kwargs:
  beam_size: 5 
  length_normalize: 0.6 
input_max_length: 150 
max_length: 150 
train_max_length: 150 # <--- SỬA QUAN TRỌNG: Tăng từ 50 lên 150. Đừng vứt bỏ câu dài!

# 4. OPTIMIZER & LEARNING RATE
lr: 2.0               # <--- SỬA: Model nhỏ (256) cần LR cao hơn để hội tụ tốt
optimizer: Adam       # <--- SỬA: Quay về Adam (chuẩn mực của Transformer). AdaBelief hơi kén chỉnh.
optimizer_params:
  betas: [0.9, 0.98]
  eps: !!float 1e-9
n_warmup_steps: 4000
label_smoothing: 0.1
dropout: 0.3          # <--- SỬA QUAN TRỌNG: Data 133k là ít, cần dropout 0.3 để chống học vẹt.

# 5. TRAINING MANAGEMENT
batch_size: 32        # <--- SỬA: Vừa vặn 8GB VRAM
accum_count: 4        # <--- THÊM: Tạo Effective Batch Size = 128 (Chuẩn)
epochs: 30            # 133k câu thì cần tầm 30 epochs
printevery: 200
save_checkpoint_epochs: 1
maximum_saved_model_eval: 5
maximum_saved_model_train: 5